{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24fea358",
   "metadata": {},
   "source": [
    "# LLM Benchmarking Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486f98f",
   "metadata": {},
   "source": [
    "## Set Up Your Python Environment\n",
    "\n",
    "To keep your project dependencies isolated and reproducible, create and activate a virtual environment before installing packages.\n",
    "\n",
    "```sh\n",
    "# Create a virtual environment\n",
    "python -m venv .venv\n",
    "\n",
    "# Activate the virtual environment (macOS/Linux)\n",
    "source .venv/bin/activate\n",
    "\n",
    "# Install required dependencies\n",
    "pip install notebook requests jsonlines pandas tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08471a79",
   "metadata": {},
   "source": [
    "## Install and Run Ollama\n",
    "\n",
    "You can install Ollama directly from the official website or run it using Docker.\n",
    "\n",
    "### Option 1: Install from the Official Website\n",
    "Download and install Ollama from: https://ollama.com/\n",
    "\n",
    "### Option 2: Use the Dockerized Version\n",
    "This approach is useful when you want a portable or containerized setup.\n",
    "\n",
    "```bash\n",
    "# Start Ollama using Docker Compose\n",
    "docker compose up -d\n",
    "\n",
    "# Enter the running Ollama container\n",
    "docker exec -it ollama bash\n",
    "```\n",
    "### Use Ollama\n",
    "\n",
    "```bash\n",
    "# Verify the Ollama installation\n",
    "ollama --version\n",
    "\n",
    "# Starts Ollama (if it isn’t already running)\n",
    "ollama serve\n",
    "\n",
    "# Print all the models that have been downloaded \n",
    "ollama list\n",
    "\n",
    "# Download a model (example: qwen:0.5b, gemma3:270m). \n",
    "ollama pull qwen:0.5b\n",
    "```\n",
    "\n",
    "To use HF models: https://huggingface.co/docs/hub/ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e8705",
   "metadata": {},
   "source": [
    "## Benchmarking Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3521db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "import requests\n",
    "import jsonlines\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# define the Ollama endpoint for the generation\n",
    "OLLAMA_GENERATE_URL = f\"http://localhost:11434/api/generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce010af",
   "metadata": {},
   "source": [
    "## Create a Function to Query the Ollama HTTP API\n",
    "\n",
    "Define a function that sends a prompt to the Ollama HTTP service and returns the model’s response.\n",
    "\n",
    "The function sets the following parameters:\n",
    "\n",
    "- **model**: the name/identifier of the Ollama model to use (e.g. `qwen:0.5b`).\n",
    "- **prompt**: the textual prompt you want to send to the model.\n",
    "- **stream** (`bool`): whether to enable streaming responses  \n",
    "  - If `True`, the model returns output incrementally (token by token).  \n",
    "  - If `False`, the function returns the full response at once.\n",
    "- **generation options**: additional parameters to control text generation, such as:\n",
    "  - `temperature` – randomness of the output.\n",
    "  - `max_tokens` – maximum number of tokens in the model’s response.\n",
    "  - (Optionally) other settings supported by the Ollama API (e.g. `top_p`, `stop` sequences).\n",
    "\n",
    "The function will construct a JSON payload with these arguments, send it to the Ollama HTTP endpoint (e.g. `POST /api/generate`), and process the response according to the `stream` mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca03102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model : str, prompt : str, options : dict = None):\n",
    "    resp = requests.post(\n",
    "        OLLAMA_GENERATE_URL,\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False, \n",
    "            \"options\": options,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # check if any error occurred during the request\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    data = resp.json()\n",
    "\n",
    "    return data[\"response\"], data[\"total_duration\"] * 10**-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1353b8ed",
   "metadata": {},
   "source": [
    "Read the prompt dataset and select one prompt to make the call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a sample prompt to test the inference function\n",
    "prompt = \"Write the dijkstra algorithm in Javascript. Output only the source code, no explaination.\"\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d953f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run \"ollama list\" to print all the models that have been downloaded\n",
    "\n",
    "# define the model to use\n",
    "OLLAMA_MODEL = \"qwen:0.5b\" # example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae271ac",
   "metadata": {},
   "source": [
    "Measure the **End-to-End Request Latency** of the inference function.\n",
    "\n",
    "This metric measures the total time from submitting a query to receiving the complete response, accounting for queueing/batching processes and network latency.\n",
    "\n",
    "More information on LLM benchmarking metrics are available here: https://docs.nvidia.com/nim/benchmarking/llm/latest/metrics.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "response, ollama_total_duration = generate(\n",
    "    model=OLLAMA_MODEL,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "e2e = end - start\n",
    "\n",
    "print(f\"Ollama total duration (from JSON response): {ollama_total_duration:.6f} s\")\n",
    "print(f\"End-to-end generation time: {e2e:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatted output\n",
    "print(f\"=== PROMPT ===\")\n",
    "print(f\"{prompt[:200]} ... \\n\")\n",
    "\n",
    "print(f\"=== RESPONSE ===\")\n",
    "print(f\"{response[:200]} ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db95bdd3",
   "metadata": {},
   "source": [
    "## Extract the Time-To-First-Token (TTFT) and Inter-Token Latency (ITL)\n",
    "\n",
    "To extract these metrics, you must enable streaming in Ollama, which allows tokens to be delivered incrementally as they are generated.\n",
    "\n",
    "We will define another function, that will have the 'stream' parameter set to True, and that is able to compute TTFT and ITL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9530c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_stream(model: str, prompt: str, options : dict = None):\n",
    "\n",
    "    # in this case we measure inside the generation function\n",
    "    start = time.time()\n",
    "\n",
    "    resp = requests.post(\n",
    "        OLLAMA_GENERATE_URL,\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": True, \n",
    "            \"options\": options,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # check if any error occurred during the request\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    first_token_time = None\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    # the iter_lines automatically checks when the stream ends\n",
    "    for line in resp.iter_lines():\n",
    "        if not line:\n",
    "            continue\n",
    "        # this time we have to use json to decode\n",
    "        data = json.loads(line.decode(\"utf-8\"))\n",
    "        token = data.get(\"response\", \"\")\n",
    "        if not token:\n",
    "            continue\n",
    "\n",
    "        # TTFT\n",
    "        now = time.time()\n",
    "        if first_token_time is None:\n",
    "            first_token_time = now\n",
    "\n",
    "        chunks.append(token)\n",
    "\n",
    "    end = time.time()\n",
    "    total_gen_time = end - start\n",
    "\n",
    "    text = \"\".join(chunks)\n",
    "\n",
    "    # the 'data' variables contains now the last json object\n",
    "    ollama_total_duration = data['total_duration'] * 10**-9\n",
    "    \n",
    "    ttft = (first_token_time - start) if first_token_time else 0.0\n",
    "    itl = (total_gen_time - ttft) / max(len(chunks) - 1, 1)\n",
    "\n",
    "\n",
    "    # in this case the response is composed of multiple json objects (you can check by directly calling the endpoint)\n",
    "\n",
    "    return text, total_gen_time, ttft, itl, ollama_total_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9cd91",
   "metadata": {},
   "source": [
    "## Run again the generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, e2e, ttft, itl, ollama_total_duration = generate_with_stream(\n",
    "    model=OLLAMA_MODEL,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "print(f\"Ollama total duration (from JSON response): {ollama_total_duration:.6f} s\")\n",
    "print(f\"End-to-end generation time: {e2e:.6f} s\")\n",
    "print(f\"Time to first token (TTFT): {ttft:.6f} s\")\n",
    "print(f\"Inter-token latency (ITL): {itl:.6f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6891467e",
   "metadata": {},
   "source": [
    "## Adjust the setup\n",
    "\n",
    "### Parameter Details\n",
    "\n",
    "`num_predict`\n",
    "Specifies the maximum number of tokens the model is allowed to generate. Lower values shorten the output, while higher values allow for longer responses.\n",
    "\n",
    "`temperature`\n",
    "Controls the randomness of the generation process. A value of 0 makes the output highly deterministic and focused.\n",
    "Higher values (e.g., 0.7–1.0) increase variability and creativity.\n",
    "Note that temperature is not the only factor influencing randomness; parameters like `top_k` and `top_p` also contribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9290893",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_OPTIONS = { \n",
    "    \"num_predict\": 256, # we can limit the maximum number of generated tokens\n",
    "    \"temperature\": 0 # handle randomness of the generation process\n",
    "}\n",
    "NUMBER_OF_ITERATIONS = 30 # define the number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "\n",
    "for it in tqdm(range(NUMBER_OF_ITERATIONS)):\n",
    "\n",
    "    response, e2e, ttft, itl, ollama_total_duration = generate_with_stream(\n",
    "        model=OLLAMA_MODEL,\n",
    "        prompt=prompt,\n",
    "        options=GENERATION_OPTIONS\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"iteration\": it,\n",
    "        \"response\": response,\n",
    "        \"ollama_total_gen_time\": ollama_total_duration,\n",
    "        \"end_to_end_latency\": e2e,\n",
    "        \"TTFT\": ttft,\n",
    "        \"ITL\": itl\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387272a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump results for further inspection and analysis \n",
    "with jsonlines.open(\"results.jsonl\", \"w\") as out_file:\n",
    "    out_file.write_all(results.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79909254",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b424485",
   "metadata": {},
   "source": [
    "## Inspect and analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "### ===> perform an in-depth examination of the value distribution\n",
    "\n",
    "def print_stats(stats):\n",
    "    for k, v in stats.items():\n",
    "        if isinstance(v, tuple):\n",
    "            print(f\"{k}: ({v[0]:.3f}, {v[1]:.3f})\")\n",
    "        else:\n",
    "            print(f\"{k}: {v:.3f}\")\n",
    "\n",
    "def summary_stats(data):\n",
    "    mu = data.mean()\n",
    "    range_ = data.max() - data.min()\n",
    "    max_dist_mean = np.abs(data - mu).max()\n",
    "    variance = data.var(ddof=1) if len(data) > 1 else 0.0\n",
    "    sigma = data.std(ddof=1) if len(data) > 1 else 0.0\n",
    "    cv = sigma / mu if mu != 0 else np.nan\n",
    "    std_error = sigma / np.sqrt(len(data)) if len(data) > 1 else 0.0\n",
    "    statistics = {\n",
    "        \"Mean (μ)\": mu,\n",
    "        \"Range\": range_,\n",
    "        \"Max distance from mean\": max_dist_mean,\n",
    "        \"Variance (σ²)\": variance,\n",
    "        \"Standard deviation (σ)\": sigma,\n",
    "        \"Coefficient of variation (CV)\": cv,\n",
    "        \"Standard error of the mean (SEM)\": std_error\n",
    "    }\n",
    "    return statistics\n",
    "\n",
    "# example on TTFT\n",
    "print_stats(summary_stats(results.TTFT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58809285",
   "metadata": {},
   "source": [
    "## Visualize the results\n",
    "\n",
    "Matplotlib and seaborn are widely used python libraries supporting a variety of plot types.\n",
    "\n",
    "```bash\n",
    "pip install matplotlib seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9030587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize the results (example)\n",
    "\n",
    "latency_measurements = results.TTFT.values\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.plot(latency_measurements, color=\"black\") \n",
    "\n",
    "plt.axhline(min(latency_measurements), color=\"lightgrey\", ls=\"--\")\n",
    "plt.axhline(max(latency_measurements), color=\"lightgrey\", ls=\"--\")\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8032ffce",
   "metadata": {},
   "source": [
    "## Run the experiment with different prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a665b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "\n",
    "# load the input prompts\n",
    "with jsonlines.open(\"sample_prompts.jsonl\", \"r\") as p_file:\n",
    "    prompts = [prompt for prompt in p_file]\n",
    "\n",
    "selected_prompts = prompts\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "\n",
    "total_iterations = len(selected_prompts) * NUMBER_OF_ITERATIONS\n",
    "pbar = tqdm(total=total_iterations, desc=\"Progress\")\n",
    "\n",
    "for prompt_idx, prompt in enumerate(selected_prompts):\n",
    "    for it in range(NUMBER_OF_ITERATIONS):\n",
    "\n",
    "        pbar.set_description(f\"Prompt {prompt_idx+1}/{len(selected_prompts)} | Iter {it+1}/{NUMBER_OF_ITERATIONS}\")\n",
    "        \n",
    "        response, ttft, itl, e2e, ollama_total_duration = generate_with_stream(\n",
    "            model=OLLAMA_MODEL,\n",
    "            prompt=prompt[\"prompt\"],\n",
    "            options=GENERATION_OPTIONS\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"prompt\": prompt[\"prompt\"],\n",
    "            \"it\": it,\n",
    "            \"ollama_total_gen_time\": ollama_total_duration,\n",
    "            \"end_to_end_latency\": e2e,\n",
    "            \"TTFT\": ttft,\n",
    "            \"ITL\": itl\n",
    "        })\n",
    "\n",
    "        pbar.update()\n",
    "pbar.close()\n",
    "\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823969e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(\"results.jsonl\", \"w\") as out_file:\n",
    "    out_file.write_all(results.to_dict('records'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
